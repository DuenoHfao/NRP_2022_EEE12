{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzh75\\AppData\\Local\\Temp\\ipykernel_25128\\3967306965.py:5: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.\n",
      "  if not df.index.is_monotonic:\n"
     ]
    },
    {
     "data": {
      "text/plain": "                       value\nDatetime                    \n2002-01-01 01:00:00  30393.0\n2002-01-01 02:00:00  29265.0\n2002-01-01 03:00:00  28357.0\n2002-01-01 04:00:00  27899.0\n2002-01-01 05:00:00  28057.0\n...                      ...\n2018-08-02 20:00:00  44057.0\n2018-08-02 21:00:00  43256.0\n2018-08-02 22:00:00  41552.0\n2018-08-02 23:00:00  38500.0\n2018-08-03 00:00:00  35486.0\n\n[145366 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2002-01-01 01:00:00</th>\n      <td>30393.0</td>\n    </tr>\n    <tr>\n      <th>2002-01-01 02:00:00</th>\n      <td>29265.0</td>\n    </tr>\n    <tr>\n      <th>2002-01-01 03:00:00</th>\n      <td>28357.0</td>\n    </tr>\n    <tr>\n      <th>2002-01-01 04:00:00</th>\n      <td>27899.0</td>\n    </tr>\n    <tr>\n      <th>2002-01-01 05:00:00</th>\n      <td>28057.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 20:00:00</th>\n      <td>44057.0</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 21:00:00</th>\n      <td>43256.0</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 22:00:00</th>\n      <td>41552.0</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 23:00:00</th>\n      <td>38500.0</td>\n    </tr>\n    <tr>\n      <th>2018-08-03 00:00:00</th>\n      <td>35486.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>145366 rows Ã— 1 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/lzh75/Downloads/archive/PJME_hourly.csv')\n",
    "\n",
    "df = df.set_index(['Datetime'])\n",
    "df.index = pd.to_datetime(df.index)\n",
    "if not df.index.is_monotonic:\n",
    "    df = df.sort_index()\n",
    "\n",
    "df = df.rename(columns={'PJME_MW': 'value'})\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzh75\\AppData\\Local\\Temp\\ipykernel_25128\\4000865392.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_n[f\"lag{n}\"] = df_n[\"value\"].shift(n)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                       value     lag1     lag2     lag3     lag4     lag5  \\\nDatetime                                                                    \n2002-01-05 05:00:00  26822.0  26669.0  27034.0  27501.0  28635.0  30924.0   \n2002-01-05 06:00:00  27399.0  26822.0  26669.0  27034.0  27501.0  28635.0   \n2002-01-05 07:00:00  28557.0  27399.0  26822.0  26669.0  27034.0  27501.0   \n2002-01-05 08:00:00  29709.0  28557.0  27399.0  26822.0  26669.0  27034.0   \n2002-01-05 09:00:00  31241.0  29709.0  28557.0  27399.0  26822.0  26669.0   \n...                      ...      ...      ...      ...      ...      ...   \n2018-08-02 20:00:00  44057.0  45641.0  46760.0  46816.0  46989.0  47154.0   \n2018-08-02 21:00:00  43256.0  44057.0  45641.0  46760.0  46816.0  46989.0   \n2018-08-02 22:00:00  41552.0  43256.0  44057.0  45641.0  46760.0  46816.0   \n2018-08-02 23:00:00  38500.0  41552.0  43256.0  44057.0  45641.0  46760.0   \n2018-08-03 00:00:00  35486.0  38500.0  41552.0  43256.0  44057.0  45641.0   \n\n                        lag6     lag7     lag8     lag9  ...    lag91  \\\nDatetime                                                 ...            \n2002-01-05 05:00:00  33202.0  35368.0  36762.0  37539.0  ...  30692.0   \n2002-01-05 06:00:00  30924.0  33202.0  35368.0  36762.0  ...  31395.0   \n2002-01-05 07:00:00  28635.0  30924.0  33202.0  35368.0  ...  31496.0   \n2002-01-05 08:00:00  27501.0  28635.0  30924.0  33202.0  ...  31031.0   \n2002-01-05 09:00:00  27034.0  27501.0  28635.0  30924.0  ...  30360.0   \n...                      ...      ...      ...      ...  ...      ...   \n2018-08-02 20:00:00  46534.0  45372.0  43954.0  42189.0  ...  28389.0   \n2018-08-02 21:00:00  47154.0  46534.0  45372.0  43954.0  ...  26779.0   \n2018-08-02 22:00:00  46989.0  47154.0  46534.0  45372.0  ...  25675.0   \n2018-08-02 23:00:00  46816.0  46989.0  47154.0  46534.0  ...  25200.0   \n2018-08-03 00:00:00  46760.0  46816.0  46989.0  47154.0  ...  25479.0   \n\n                       lag92    lag93    lag94    lag95    lag96    lag97  \\\nDatetime                                                                    \n2002-01-05 05:00:00  29943.0  29595.0  29308.0  28654.0  28057.0  27899.0   \n2002-01-05 06:00:00  30692.0  29943.0  29595.0  29308.0  28654.0  28057.0   \n2002-01-05 07:00:00  31395.0  30692.0  29943.0  29595.0  29308.0  28654.0   \n2002-01-05 08:00:00  31496.0  31395.0  30692.0  29943.0  29595.0  29308.0   \n2002-01-05 09:00:00  31031.0  31496.0  31395.0  30692.0  29943.0  29595.0   \n...                      ...      ...      ...      ...      ...      ...   \n2018-08-02 20:00:00  30789.0  33747.0  36581.0  37870.0  39089.0  40517.0   \n2018-08-02 21:00:00  28389.0  30789.0  33747.0  36581.0  37870.0  39089.0   \n2018-08-02 22:00:00  26779.0  28389.0  30789.0  33747.0  36581.0  37870.0   \n2018-08-02 23:00:00  25675.0  26779.0  28389.0  30789.0  33747.0  36581.0   \n2018-08-03 00:00:00  25200.0  25675.0  26779.0  28389.0  30789.0  33747.0   \n\n                       lag98    lag99   lag100  \nDatetime                                        \n2002-01-05 05:00:00  28357.0  29265.0  30393.0  \n2002-01-05 06:00:00  27899.0  28357.0  29265.0  \n2002-01-05 07:00:00  28057.0  27899.0  28357.0  \n2002-01-05 08:00:00  28654.0  28057.0  27899.0  \n2002-01-05 09:00:00  29308.0  28654.0  28057.0  \n...                      ...      ...      ...  \n2018-08-02 20:00:00  40709.0  39906.0  38637.0  \n2018-08-02 21:00:00  40517.0  40709.0  39906.0  \n2018-08-02 22:00:00  39089.0  40517.0  40709.0  \n2018-08-02 23:00:00  37870.0  39089.0  40517.0  \n2018-08-03 00:00:00  36581.0  37870.0  39089.0  \n\n[145266 rows x 101 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value</th>\n      <th>lag1</th>\n      <th>lag2</th>\n      <th>lag3</th>\n      <th>lag4</th>\n      <th>lag5</th>\n      <th>lag6</th>\n      <th>lag7</th>\n      <th>lag8</th>\n      <th>lag9</th>\n      <th>...</th>\n      <th>lag91</th>\n      <th>lag92</th>\n      <th>lag93</th>\n      <th>lag94</th>\n      <th>lag95</th>\n      <th>lag96</th>\n      <th>lag97</th>\n      <th>lag98</th>\n      <th>lag99</th>\n      <th>lag100</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2002-01-05 05:00:00</th>\n      <td>26822.0</td>\n      <td>26669.0</td>\n      <td>27034.0</td>\n      <td>27501.0</td>\n      <td>28635.0</td>\n      <td>30924.0</td>\n      <td>33202.0</td>\n      <td>35368.0</td>\n      <td>36762.0</td>\n      <td>37539.0</td>\n      <td>...</td>\n      <td>30692.0</td>\n      <td>29943.0</td>\n      <td>29595.0</td>\n      <td>29308.0</td>\n      <td>28654.0</td>\n      <td>28057.0</td>\n      <td>27899.0</td>\n      <td>28357.0</td>\n      <td>29265.0</td>\n      <td>30393.0</td>\n    </tr>\n    <tr>\n      <th>2002-01-05 06:00:00</th>\n      <td>27399.0</td>\n      <td>26822.0</td>\n      <td>26669.0</td>\n      <td>27034.0</td>\n      <td>27501.0</td>\n      <td>28635.0</td>\n      <td>30924.0</td>\n      <td>33202.0</td>\n      <td>35368.0</td>\n      <td>36762.0</td>\n      <td>...</td>\n      <td>31395.0</td>\n      <td>30692.0</td>\n      <td>29943.0</td>\n      <td>29595.0</td>\n      <td>29308.0</td>\n      <td>28654.0</td>\n      <td>28057.0</td>\n      <td>27899.0</td>\n      <td>28357.0</td>\n      <td>29265.0</td>\n    </tr>\n    <tr>\n      <th>2002-01-05 07:00:00</th>\n      <td>28557.0</td>\n      <td>27399.0</td>\n      <td>26822.0</td>\n      <td>26669.0</td>\n      <td>27034.0</td>\n      <td>27501.0</td>\n      <td>28635.0</td>\n      <td>30924.0</td>\n      <td>33202.0</td>\n      <td>35368.0</td>\n      <td>...</td>\n      <td>31496.0</td>\n      <td>31395.0</td>\n      <td>30692.0</td>\n      <td>29943.0</td>\n      <td>29595.0</td>\n      <td>29308.0</td>\n      <td>28654.0</td>\n      <td>28057.0</td>\n      <td>27899.0</td>\n      <td>28357.0</td>\n    </tr>\n    <tr>\n      <th>2002-01-05 08:00:00</th>\n      <td>29709.0</td>\n      <td>28557.0</td>\n      <td>27399.0</td>\n      <td>26822.0</td>\n      <td>26669.0</td>\n      <td>27034.0</td>\n      <td>27501.0</td>\n      <td>28635.0</td>\n      <td>30924.0</td>\n      <td>33202.0</td>\n      <td>...</td>\n      <td>31031.0</td>\n      <td>31496.0</td>\n      <td>31395.0</td>\n      <td>30692.0</td>\n      <td>29943.0</td>\n      <td>29595.0</td>\n      <td>29308.0</td>\n      <td>28654.0</td>\n      <td>28057.0</td>\n      <td>27899.0</td>\n    </tr>\n    <tr>\n      <th>2002-01-05 09:00:00</th>\n      <td>31241.0</td>\n      <td>29709.0</td>\n      <td>28557.0</td>\n      <td>27399.0</td>\n      <td>26822.0</td>\n      <td>26669.0</td>\n      <td>27034.0</td>\n      <td>27501.0</td>\n      <td>28635.0</td>\n      <td>30924.0</td>\n      <td>...</td>\n      <td>30360.0</td>\n      <td>31031.0</td>\n      <td>31496.0</td>\n      <td>31395.0</td>\n      <td>30692.0</td>\n      <td>29943.0</td>\n      <td>29595.0</td>\n      <td>29308.0</td>\n      <td>28654.0</td>\n      <td>28057.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 20:00:00</th>\n      <td>44057.0</td>\n      <td>45641.0</td>\n      <td>46760.0</td>\n      <td>46816.0</td>\n      <td>46989.0</td>\n      <td>47154.0</td>\n      <td>46534.0</td>\n      <td>45372.0</td>\n      <td>43954.0</td>\n      <td>42189.0</td>\n      <td>...</td>\n      <td>28389.0</td>\n      <td>30789.0</td>\n      <td>33747.0</td>\n      <td>36581.0</td>\n      <td>37870.0</td>\n      <td>39089.0</td>\n      <td>40517.0</td>\n      <td>40709.0</td>\n      <td>39906.0</td>\n      <td>38637.0</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 21:00:00</th>\n      <td>43256.0</td>\n      <td>44057.0</td>\n      <td>45641.0</td>\n      <td>46760.0</td>\n      <td>46816.0</td>\n      <td>46989.0</td>\n      <td>47154.0</td>\n      <td>46534.0</td>\n      <td>45372.0</td>\n      <td>43954.0</td>\n      <td>...</td>\n      <td>26779.0</td>\n      <td>28389.0</td>\n      <td>30789.0</td>\n      <td>33747.0</td>\n      <td>36581.0</td>\n      <td>37870.0</td>\n      <td>39089.0</td>\n      <td>40517.0</td>\n      <td>40709.0</td>\n      <td>39906.0</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 22:00:00</th>\n      <td>41552.0</td>\n      <td>43256.0</td>\n      <td>44057.0</td>\n      <td>45641.0</td>\n      <td>46760.0</td>\n      <td>46816.0</td>\n      <td>46989.0</td>\n      <td>47154.0</td>\n      <td>46534.0</td>\n      <td>45372.0</td>\n      <td>...</td>\n      <td>25675.0</td>\n      <td>26779.0</td>\n      <td>28389.0</td>\n      <td>30789.0</td>\n      <td>33747.0</td>\n      <td>36581.0</td>\n      <td>37870.0</td>\n      <td>39089.0</td>\n      <td>40517.0</td>\n      <td>40709.0</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 23:00:00</th>\n      <td>38500.0</td>\n      <td>41552.0</td>\n      <td>43256.0</td>\n      <td>44057.0</td>\n      <td>45641.0</td>\n      <td>46760.0</td>\n      <td>46816.0</td>\n      <td>46989.0</td>\n      <td>47154.0</td>\n      <td>46534.0</td>\n      <td>...</td>\n      <td>25200.0</td>\n      <td>25675.0</td>\n      <td>26779.0</td>\n      <td>28389.0</td>\n      <td>30789.0</td>\n      <td>33747.0</td>\n      <td>36581.0</td>\n      <td>37870.0</td>\n      <td>39089.0</td>\n      <td>40517.0</td>\n    </tr>\n    <tr>\n      <th>2018-08-03 00:00:00</th>\n      <td>35486.0</td>\n      <td>38500.0</td>\n      <td>41552.0</td>\n      <td>43256.0</td>\n      <td>44057.0</td>\n      <td>45641.0</td>\n      <td>46760.0</td>\n      <td>46816.0</td>\n      <td>46989.0</td>\n      <td>47154.0</td>\n      <td>...</td>\n      <td>25479.0</td>\n      <td>25200.0</td>\n      <td>25675.0</td>\n      <td>26779.0</td>\n      <td>28389.0</td>\n      <td>30789.0</td>\n      <td>33747.0</td>\n      <td>36581.0</td>\n      <td>37870.0</td>\n      <td>39089.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>145266 rows Ã— 101 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_time_lags(df, n_lags):\n",
    "    df_n = df.copy()\n",
    "    for n in range(1, n_lags + 1):\n",
    "        df_n[f\"lag{n}\"] = df_n[\"value\"].shift(n)\n",
    "    df_n = df_n.iloc[n_lags:]\n",
    "    return df_n\n",
    "\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "df_generated = generate_time_lags(df, input_dim)\n",
    "df_generated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzh75\\AppData\\Local\\Temp\\ipykernel_25128\\3711351413.py:7: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  .assign(week_of_year = df.index.week)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                       value  hour  day  month  day_of_week  week_of_year\nDatetime                                                                 \n2002-01-01 01:00:00  30393.0     1    1      1            1             1\n2002-01-01 02:00:00  29265.0     2    1      1            1             1\n2002-01-01 03:00:00  28357.0     3    1      1            1             1\n2002-01-01 04:00:00  27899.0     4    1      1            1             1\n2002-01-01 05:00:00  28057.0     5    1      1            1             1\n...                      ...   ...  ...    ...          ...           ...\n2018-08-02 20:00:00  44057.0    20    2      8            3            31\n2018-08-02 21:00:00  43256.0    21    2      8            3            31\n2018-08-02 22:00:00  41552.0    22    2      8            3            31\n2018-08-02 23:00:00  38500.0    23    2      8            3            31\n2018-08-03 00:00:00  35486.0     0    3      8            4            31\n\n[145366 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>value</th>\n      <th>hour</th>\n      <th>day</th>\n      <th>month</th>\n      <th>day_of_week</th>\n      <th>week_of_year</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2002-01-01 01:00:00</th>\n      <td>30393.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2002-01-01 02:00:00</th>\n      <td>29265.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2002-01-01 03:00:00</th>\n      <td>28357.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2002-01-01 04:00:00</th>\n      <td>27899.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2002-01-01 05:00:00</th>\n      <td>28057.0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 20:00:00</th>\n      <td>44057.0</td>\n      <td>20</td>\n      <td>2</td>\n      <td>8</td>\n      <td>3</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 21:00:00</th>\n      <td>43256.0</td>\n      <td>21</td>\n      <td>2</td>\n      <td>8</td>\n      <td>3</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 22:00:00</th>\n      <td>41552.0</td>\n      <td>22</td>\n      <td>2</td>\n      <td>8</td>\n      <td>3</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2018-08-02 23:00:00</th>\n      <td>38500.0</td>\n      <td>23</td>\n      <td>2</td>\n      <td>8</td>\n      <td>3</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>2018-08-03 00:00:00</th>\n      <td>35486.0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>8</td>\n      <td>4</td>\n      <td>31</td>\n    </tr>\n  </tbody>\n</table>\n<p>145366 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = (\n",
    "    df\n",
    "    .assign(hour=df.index.hour)\n",
    "    .assign(day=df.index.day)\n",
    "    .assign(month=df.index.month)\n",
    "    .assign(day_of_week=df.index.dayofweek)\n",
    "    .assign(week_of_year=df.index.week)\n",
    ")\n",
    "df_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of 'prefix' (4) did not match the length of the columns being encoded (0).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m     dummies \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(df[col_name], prefix\u001B[38;5;241m=\u001B[39mcol_name)\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mconcat([df, dummies], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[col_name])\n\u001B[1;32m----> 5\u001B[0m df_features \u001B[38;5;241m=\u001B[39m \u001B[43monehot_encode_pd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmonth\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mday\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mday_of_week\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweek_of_year\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[23], line 2\u001B[0m, in \u001B[0;36monehot_encode_pd\u001B[1;34m(df, col_name)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21monehot_encode_pd\u001B[39m(df, col_name):\n\u001B[1;32m----> 2\u001B[0m     dummies \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dummies\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcol_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mconcat([df, dummies], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[col_name])\n",
      "File \u001B[1;32mD:\\coding\\project\\NRP_2022_EEE12\\venv\\lib\\site-packages\\pandas\\core\\reshape\\encoding.py:160\u001B[0m, in \u001B[0;36mget_dummies\u001B[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001B[0m\n\u001B[0;32m    153\u001B[0m             len_msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    154\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(item)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) did not match the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    155\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength of the columns being encoded \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    156\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_to_encode\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    157\u001B[0m             )\n\u001B[0;32m    158\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(len_msg)\n\u001B[1;32m--> 160\u001B[0m \u001B[43mcheck_len\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprefix\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    161\u001B[0m check_len(prefix_sep, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprefix_sep\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prefix, \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[1;32mD:\\coding\\project\\NRP_2022_EEE12\\venv\\lib\\site-packages\\pandas\\core\\reshape\\encoding.py:158\u001B[0m, in \u001B[0;36mget_dummies.<locals>.check_len\u001B[1;34m(item, name)\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(item) \u001B[38;5;241m==\u001B[39m data_to_encode\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]:\n\u001B[0;32m    153\u001B[0m     len_msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    154\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(item)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) did not match the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    155\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength of the columns being encoded \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    156\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_to_encode\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    157\u001B[0m     )\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(len_msg)\n",
      "\u001B[1;31mValueError\u001B[0m: Length of 'prefix' (4) did not match the length of the columns being encoded (0)."
     ]
    }
   ],
   "source": [
    "def onehot_encode_pd(df, col_name):\n",
    "    dummies = pd.get_dummies(df[col_name], prefix=col_name)\n",
    "    return pd.concat([df, dummies], axis=1).drop(columns=[col_name])\n",
    "\n",
    "\n",
    "df_features = onehot_encode_pd(df_features, ['month', 'day', 'day_of_week', 'week_of_year'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def feature_label_split(df, target_col):\n",
    "    y = df[[target_col]]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_val_test_split(df, target_col, test_ratio):\n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X, y = feature_label_split(df, target_col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df_features, 'value', 0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_val_arr = scaler.transform(X_val)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train)\n",
    "y_val_arr = scaler.transform(y_val)\n",
    "y_test_arr = scaler.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr)\n",
    "train_targets = torch.Tensor(y_train_arr)\n",
    "val_features = torch.Tensor(X_val_arr)\n",
    "val_targets = torch.Tensor(y_val_arr)\n",
    "test_features = torch.Tensor(X_test_arr)\n",
    "test_targets = torch.Tensor(y_test_arr)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "val = TensorDataset(val_features, val_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.layer_dim = layer_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "        model_path = f'models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().numpy())\n",
    "                values.append(y_test.to(device).detach().numpy())\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
